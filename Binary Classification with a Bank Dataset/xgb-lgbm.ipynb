{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-29T02:09:14.682970Z","iopub.execute_input":"2025-08-29T02:09:14.683146Z","iopub.status.idle":"2025-08-29T02:09:16.392961Z","shell.execute_reply.started":"2025-08-29T02:09:14.683129Z","shell.execute_reply":"2025-08-29T02:09:16.392255Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e8/sample_submission.csv\n/kaggle/input/playground-series-s5e8/train.csv\n/kaggle/input/playground-series-s5e8/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nimport polars as pl\n\nBASE = Path(\"/kaggle/input/playground-series-s5e8\")\ntrain, test, sub = (pl.read_csv(BASE / f) for f in (\"train.csv\", \"test.csv\", \"sample_submission.csv\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T02:09:46.149883Z","iopub.execute_input":"2025-08-29T02:09:46.150117Z","iopub.status.idle":"2025-08-29T02:09:47.687206Z","shell.execute_reply.started":"2025-08-29T02:09:46.150098Z","shell.execute_reply":"2025-08-29T02:09:47.686573Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ===== XGBoost (Faster, Fixed) — CV + EarlyStopping(in constructor) + CPU/GPU auto =====\nfrom pathlib import Path\nimport os, warnings, subprocess\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import __version__ as sklearn_version\nfrom packaging import version\nfrom joblib import Parallel, delayed\n\nimport xgboost as xgb\n\n# ----------------------- Config -----------------------\nBASE = Path(\"/kaggle/input/playground-series-s5e8\")\nTARGET, ID = \"y\", \"id\"\nDROP_DURATION = True          # 现实部署建议 True（duration 有潜在泄漏）\nFOLDS = 5\nSEED = 42\nEARLY_STOP = 200\nVERBOSE = False\n\n# —— 极速模式（可能略降分；提速明显）——\nULTRA_FAST = False\n\n# 外层并行（多核时 2 折并行较稳；核少时自动关）\nCPU = os.cpu_count() or 8\nOUTER_JOBS = 2 if CPU >= 16 else 1\nINNER_JOBS = max(1, CPU // OUTER_JOBS)\n\n# ----------------------- 环境/版本探测 -----------------------\ndef has_gpu() -> bool:\n    try:\n        out = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n        return out.returncode == 0 and \"GPU\" in out.stdout\n    except Exception:\n        return False\n\nXGB_VER = version.parse(xgb.__version__)\nHAS_GPU = has_gpu()\n\n# XGBoost 2.0+：用 device；老版本：用 tree_method\nif XGB_VER >= version.parse(\"2.0.0\"):\n    DEVICE_KW = dict(tree_method=\"hist\", device=(\"cuda\" if HAS_GPU else \"cpu\"))\nelse:\n    DEVICE_KW = dict(tree_method=(\"gpu_hist\" if HAS_GPU else \"hist\"))\n\n# sklearn 1.2+ 的 OneHotEncoder 使用 sparse_output\nHAS_SPARSE_OUTPUT = version.parse(sklearn_version) >= version.parse(\"1.2\")\n\n# XGB 原生类别支持（1.6+）\nHAS_NATIVE_CAT = XGB_VER >= version.parse(\"1.6.0\")\n\n# ----------------------- Load -------------------------\ntrain_pl = pl.read_csv(BASE / \"train.csv\")\ntest_pl  = pl.read_csv(BASE / \"test.csv\")\nsub      = pd.read_csv(BASE / \"sample_submission.csv\")\n\nif DROP_DURATION and \"duration\" in train_pl.columns:\n    train_pl = train_pl.drop(\"duration\")\n    if \"duration\" in test_pl.columns:\n        test_pl = test_pl.drop(\"duration\")\n\ntrain = train_pl.to_pandas()\ntest  = test_pl.to_pandas()\n\ny = train[TARGET].astype(int).values\nX = train.drop(columns=[TARGET, ID])\nX_test = test.drop(columns=[ID], errors=\"ignore\")\n\n# ----------------------- 轻量特征工程 & 同步到测试集 -----------------------\n# 1) pdays：-1 表示从未联系过\nif \"pdays\" in X.columns:\n    X[\"was_prev_contacted\"] = (X[\"pdays\"] != -1).astype(np.int8)\n    X_test[\"was_prev_contacted\"] = (X_test[\"pdays\"] != -1).astype(np.int8)\n    X[\"pdays_pos\"] = X[\"pdays\"].where(X[\"pdays\"] >= 0, other=np.nan)\n    X_test[\"pdays_pos\"] = X_test[\"pdays\"].where(X_test[\"pdays\"] >= 0, other=np.nan)\n\n# 2) month 周期（同步创建到测试集）\nmonth_map = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\n             \"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\nif \"month\" in X.columns:\n    X[\"month_num\"] = pd.Series(X[\"month\"]).map(month_map).fillna(0).astype(int)\n    X_test[\"month_num\"] = pd.Series(X_test[\"month\"]).map(month_map).fillna(0).astype(int)\n    X[\"month_sin\"] = np.sin(2*np.pi*X[\"month_num\"]/12)\n    X[\"month_cos\"] = np.cos(2*np.pi*X[\"month_num\"]/12)\n    X_test[\"month_sin\"] = np.sin(2*np.pi*X_test[\"month_num\"]/12)\n    X_test[\"month_cos\"] = np.cos(2*np.pi*X_test[\"month_num\"]/12)\n\n# 3) 温和去极值\nfor col in [\"balance\", \"campaign\", \"previous\"]:\n    if col in X.columns:\n        lo, hi = X[col].quantile([0.001, 0.999])\n        X[col] = X[col].clip(lo, hi)\n        if col in X_test.columns:\n            X_test[col] = X_test[col].clip(lo, hi)\n\n# —— 统一降精度（先对齐列，防止测试集缺列报 KeyError）——\nfor c in X.columns:\n    if c not in X_test.columns:\n        X_test[c] = np.nan\n\nfor c in X.columns:\n    if str(X[c].dtype) == \"category\":\n        continue\n    if pd.api.types.is_integer_dtype(X[c]):\n        X[c] = X[c].astype(np.int32)\n        X_test[c] = X_test[c].astype(np.int32, errors=\"ignore\")\n    elif pd.api.types.is_float_dtype(X[c]) or pd.api.types.is_bool_dtype(X[c]):\n        X[c] = X[c].astype(np.float32)\n        X_test[c] = X_test[c].astype(np.float32, errors=\"ignore\")\n\n# —— 将 object 转为 category（原生类别更快/更省内存；One-Hot 也可用）——\nfor c in X.columns:\n    if X[c].dtype == \"object\":\n        X[c] = X[c].astype(\"category\")\nfor c in X.columns:\n    if str(X[c].dtype) == \"category\" and c in X_test.columns:\n        X_test[c] = X_test[c].astype(\"category\")\n\n# 最终对齐列顺序\nX_test = X_test.reindex(columns=X.columns)\n\n# 列分组\ndef is_cat(s: pd.Series) -> bool:\n    return (s.dtype == \"object\") or (str(s.dtype) == \"category\")\ncat_cols = [c for c in X.columns if is_cat(X[c])]\nnum_cols = [c for c in X.columns if c not in cat_cols]\n\n# ----------------------- Params -----------------------\npos = y.sum(); neg = len(y) - pos\nscale_pos_weight = neg / max(1, pos)\n\nbase_params = dict(\n    objective=\"binary:logistic\",\n    eval_metric=\"auc\",\n    learning_rate=0.03,\n    n_estimators=2000,            # 会被 early stopping 截断\n    max_depth=6,\n    min_child_weight=4,\n    subsample=0.85,\n    colsample_bytree=0.85,\n    colsample_bynode=0.85,\n    reg_alpha=0.0,\n    reg_lambda=1.0,\n    max_bin=512,\n    random_state=SEED,\n    n_jobs=INNER_JOBS,            # 外层并行时限制内层线程数\n    scale_pos_weight=scale_pos_weight,\n    early_stopping_rounds=EARLY_STOP,   # 在构造器里传\n    **DEVICE_KW,                        # 正确设定 CPU/GPU\n)\n\n# 更快的直方图设置（基本不掉分）\nbase_params.update(dict(\n    max_bin=256,\n    colsample_bytree=0.75,\n    colsample_bynode=0.75,\n    subsample=0.80,\n    min_child_weight=6,\n))\nif XGB_VER >= version.parse(\"1.6.0\"):\n    base_params[\"sampling_method\"] = \"gradient_based\"\n\n# ✅ 仅对 XGBoost < 2.0 且有 GPU 时设置 predictor，避免 2.x 的“unused parameter”警告\nif XGB_VER < version.parse(\"2.0.0\") and HAS_GPU:\n    base_params[\"predictor\"] = \"gpu_predictor\"\n\n# 极速模式（可能略降分）\nif ULTRA_FAST:\n    FOLDS = 3\n    EARLY_STOP = 100\n    base_params.update(dict(\n        learning_rate=0.05,\n        max_depth=5,\n        min_child_weight=8,\n        subsample=0.75,\n        colsample_bytree=0.70,\n        colsample_bynode=0.70,\n        n_estimators=1500,\n        max_bin=256,\n    ))\n    base_params[\"early_stopping_rounds\"] = EARLY_STOP\n\n# ----------------------- Encoders (One-Hot fallback prefit) -----------------------\npre = None\nif not HAS_NATIVE_CAT and len(cat_cols) > 0:\n    if HAS_SPARSE_OUTPUT:\n        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, dtype=np.float32)\n    else:\n        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True, dtype=np.float32)\n    pre = ColumnTransformer([(\"cat\", ohe, cat_cols)], remainder=\"passthrough\")\n    pre.fit(X)  # 预先全量拟合（不依赖 y），各折复用\n\n# ----------------------- Helpers -----------------------\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\ndef best_iter_of(model: xgb.XGBClassifier) -> int:\n    if hasattr(model, \"best_iteration_\") and model.best_iteration_ is not None:\n        return int(model.best_iteration_)\n    try:\n        return int(model.get_booster().best_ntree_limit)\n    except Exception:\n        return int(getattr(model, \"n_estimators\", 2000))\n\ndef run_fold(tr_idx, va_idx):\n    if HAS_NATIVE_CAT:\n        params = base_params | dict(enable_categorical=True)\n        model = xgb.XGBClassifier(**params)\n        model.fit(X.iloc[tr_idx], y[tr_idx], eval_set=[(X.iloc[va_idx], y[va_idx])], verbose=VERBOSE)\n        proba_va = model.predict_proba(X.iloc[va_idx])[:, 1]\n    else:\n        Xtr = pre.transform(X.iloc[tr_idx]) if pre is not None else X.iloc[tr_idx]\n        Xva = pre.transform(X.iloc[va_idx]) if pre is not None else X.iloc[va_idx]\n        model = xgb.XGBClassifier(**base_params)\n        model.fit(Xtr, y[tr_idx], eval_set=[(Xva, y[va_idx])], verbose=VERBOSE)\n        proba_va = model.predict_proba(Xva)[:, 1]\n    return va_idx, proba_va, model\n\n# ----------------------- OOF CV（可外层并行） -----------------------\nfolds = list(skf.split(X, y))\nif OUTER_JOBS > 1:\n    results = Parallel(n_jobs=OUTER_JOBS, prefer=\"threads\")(delayed(run_fold)(tr, va) for tr, va in folds)\nelse:\n    results = [run_fold(tr, va) for tr, va in folds]\n\noof = np.zeros(len(y), dtype=float)\nbest_iters = []\nfor va_idx, proba_va, model in results:\n    oof[va_idx] = proba_va\n    bi = best_iter_of(model)\n    best_iters.append(bi)\n    print(f\"[Fold] best_iter={bi}  AUC={roc_auc_score(y[va_idx], oof[va_idx]):.4f}\")\n\nauc = roc_auc_score(y, oof)\nf1_05 = f1_score(y, (oof >= 0.5).astype(int))\nprec, rec, thr = precision_recall_curve(y, oof)\nf1s = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-12)\nbest_ix = int(np.nanargmax(f1s))\nbest_thr = float(thr[best_ix])\nprint(f\"[OOF] AUC={auc:.4f} | F1@0.5={f1_05:.4f} | BestF1={f1s[best_ix]:.4f} @thr={best_thr:.4f}\")\n\n# ----------------------- Full fit with tuned n_estimators -----------------------\nbest_round = max(100, int(np.median(best_iters)))\nprint(f\"[Full] training with n_estimators={best_round} (median of CV best iters)\")\n\nif HAS_NATIVE_CAT:\n    params_full = base_params | dict(enable_categorical=True, n_estimators=best_round)\n    final = xgb.XGBClassifier(**params_full)\n    # 用随机 5% 作为早停验证（更快；若不稳定可改回 10%）\n    rng = np.random.RandomState(SEED)\n    idx = rng.permutation(len(X))\n    cut = int(0.95 * len(X))\n    tr_idx, va_idx = idx[:cut], idx[cut:]\n    final.fit(X.iloc[tr_idx], y[tr_idx], eval_set=[(X.iloc[va_idx], y[va_idx])], verbose=VERBOSE)\n    test_proba = final.predict_proba(X_test)[:, 1]\nelse:\n    if pre is None:\n        Xt_all, Xt_tst = X, X_test\n    else:\n        Xt_all = pre.transform(X)\n        Xt_tst = pre.transform(X_test)\n    params_full = base_params | dict(n_estimators=best_round)\n    final = xgb.XGBClassifier(**params_full)\n\n    rng = np.random.RandomState(SEED)\n    idx = rng.permutation(len(X))\n    cut = int(0.95 * len(X))\n    tr_idx, va_idx = idx[:cut], idx[cut:]\n\n    X_tr, y_tr = Xt_all[tr_idx], y[tr_idx]\n    X_va, y_va = Xt_all[va_idx], y[va_idx]\n    final.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=VERBOSE)\n    test_proba = final.predict_proba(Xt_tst)[:, 1]\n\n# ----------------------- Submit -----------------------\nout = sub.copy()\nout[\"y\"] = test_proba\nout.to_csv(\"submission_xgb_sota.csv\", index=False)\nprint(\"Saved -> submission_xgb_sota.csv\")\nprint(out.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T02:19:42.196885Z","iopub.execute_input":"2025-08-29T02:19:42.197183Z","iopub.status.idle":"2025-08-29T02:22:04.923751Z","shell.execute_reply.started":"2025-08-29T02:19:42.197164Z","shell.execute_reply":"2025-08-29T02:22:04.922991Z"}},"outputs":[{"name":"stdout","text":"[Fold] best_iter=2000  AUC=0.8598\n[Fold] best_iter=2000  AUC=0.8553\n[Fold] best_iter=2000  AUC=0.8550\n[Fold] best_iter=2000  AUC=0.8570\n[Fold] best_iter=2000  AUC=0.8544\n[OOF] AUC=0.8563 | F1@0.5=0.4713 | BestF1=0.5310 @thr=0.7031\n[Full] training with n_estimators=2000 (median of CV best iters)\nSaved -> submission_xgb_sota.csv\n       id         y\n0  750000  0.231733\n1  750001  0.349984\n2  750002  0.356784\n3  750003  0.001271\n4  750004  0.618832\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ===== LightGBM (Fast, Fixed) — CV + EarlyStopping + CPU/GPU auto =====\nfrom pathlib import Path\nimport os, warnings, subprocess\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve\nfrom joblib import Parallel, delayed\n\nimport lightgbm as lgb\nfrom packaging import version\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------- Config -----------------------\nBASE = Path(\"/kaggle/input/playground-series-s5e8\")\nTARGET, ID = \"y\", \"id\"\nDROP_DURATION = True          # 现实部署建议 True（duration 有潜在泄漏）\nFOLDS = 5\nSEED = 42\nEARLY_STOP = 200\nVERBOSE = False               # 控制训练日志\n\n# —— 极速模式（可能略降分；提速明显）——\nULTRA_FAST = False\n\n# 外层并行（多核时 2 折并行较稳；核少时自动关）\nCPU = os.cpu_count() or 8\nOUTER_JOBS = 2 if CPU >= 16 else 1\nINNER_JOBS = max(1, CPU // OUTER_JOBS)\n\n# ----------------------- 环境/版本探测 -----------------------\ndef has_gpu() -> bool:\n    try:\n        out = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n        return out.returncode == 0 and \"GPU\" in out.stdout\n    except Exception:\n        return False\n\nHAS_GPU = has_gpu()\nLGB_VER = version.parse(lgb.__version__)\n\n# LightGBM 设备参数\nDEVICE_KW = dict(device_type=(\"gpu\" if HAS_GPU else \"cpu\"))\n\n# ----------------------- Load -------------------------\ntrain_pl = pl.read_csv(BASE / \"train.csv\")\ntest_pl  = pl.read_csv(BASE / \"test.csv\")\nsub      = pd.read_csv(BASE / \"sample_submission.csv\")\n\nif DROP_DURATION and \"duration\" in train_pl.columns:\n    train_pl = train_pl.drop(\"duration\")\n    if \"duration\" in test_pl.columns:\n        test_pl = test_pl.drop(\"duration\")\n\ntrain = train_pl.to_pandas()\ntest  = test_pl.to_pandas()\n\ny = train[TARGET].astype(int).values\nX = train.drop(columns=[TARGET, ID])\nX_test = test.drop(columns=[ID], errors=\"ignore\")\n\n# ----------------------- 轻量特征工程 & 同步到测试集 -----------------------\n# 1) pdays：-1 表示从未联系过\nif \"pdays\" in X.columns:\n    X[\"was_prev_contacted\"] = (X[\"pdays\"] != -1).astype(np.int8)\n    X_test[\"was_prev_contacted\"] = (X_test[\"pdays\"] != -1).astype(np.int8)\n    X[\"pdays_pos\"] = X[\"pdays\"].where(X[\"pdays\"] >= 0, other=np.nan)\n    X_test[\"pdays_pos\"] = X_test[\"pdays\"].where(X_test[\"pdays\"] >= 0, other=np.nan)\n\n# 2) month 周期\nmonth_map = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\n             \"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\nif \"month\" in X.columns:\n    X[\"month_num\"] = pd.Series(X[\"month\"]).map(month_map).fillna(0).astype(int)\n    X_test[\"month_num\"] = pd.Series(X_test[\"month\"]).map(month_map).fillna(0).astype(int)\n    X[\"month_sin\"] = np.sin(2*np.pi*X[\"month_num\"]/12)\n    X[\"month_cos\"] = np.cos(2*np.pi*X[\"month_num\"]/12)\n    X_test[\"month_sin\"] = np.sin(2*np.pi*X_test[\"month_num\"]/12)\n    X_test[\"month_cos\"] = np.cos(2*np.pi*X_test[\"month_num\"]/12)\n\n# 3) 温和去极值\nfor col in [\"balance\", \"campaign\", \"previous\"]:\n    if col in X.columns:\n        lo, hi = X[col].quantile([0.001, 0.999])\n        X[col] = X[col].clip(lo, hi)\n        if col in X_test.columns:\n            X_test[col] = X_test[col].clip(lo, hi)\n\n# —— 统一降精度（先对齐列，防止测试集缺列报错）——\nfor c in X.columns:\n    if c not in X_test.columns:\n        X_test[c] = np.nan\n\nfor c in X.columns:\n    if str(X[c].dtype) == \"category\":\n        continue\n    if pd.api.types.is_integer_dtype(X[c]):\n        X[c] = X[c].astype(np.int32)\n        X_test[c] = X_test[c].astype(np.int32, errors=\"ignore\")\n    elif pd.api.types.is_float_dtype(X[c]) or pd.api.types.is_bool_dtype(X[c]):\n        X[c] = X[c].astype(np.float32)\n        X_test[c] = X_test[c].astype(np.float32, errors=\"ignore\")\n\n# —— 将 object 转为 category（LightGBM 原生类别支持，速度/内存友好）——\nfor c in X.columns:\n    if X[c].dtype == \"object\":\n        X[c] = X[c].astype(\"category\")\nfor c in X.columns:\n    if str(X[c].dtype) == \"category\" and c in X_test.columns:\n        X_test[c] = X_test[c].astype(\"category\")\n\n# 最终对齐列顺序\nX_test = X_test.reindex(columns=X.columns)\n\n# 类别列/数值列\ndef is_cat(s: pd.Series) -> bool:\n    return (s.dtype == \"object\") or (str(s.dtype) == \"category\")\n\ncat_cols = [c for c in X.columns if is_cat(X[c])]\nnum_cols = [c for c in X.columns if c not in cat_cols]\n\n# ----------------------- Params -----------------------\npos = y.sum(); neg = len(y) - pos\nscale_pos_weight = neg / max(1, pos)\n\nbase_params = dict(\n    objective=\"binary\",\n    metric=\"auc\",\n    learning_rate=0.03,\n    n_estimators=2000,            # 会被 early stopping 截断\n    num_leaves=63,                # ~ 2^6-1（对应 max_depth≈6）\n    max_depth=-1,                 # 由 num_leaves 控制复杂度\n    min_data_in_leaf=40,\n    feature_fraction=0.85,\n    bagging_fraction=0.85,\n    bagging_freq=1,\n    lambda_l1=0.0,\n    lambda_l2=1.0,\n    max_bin=255,\n    random_state=SEED,\n    n_jobs=INNER_JOBS,            # ✅ 外层并行时限制内层线程数，避免超订\n    scale_pos_weight=scale_pos_weight,\n    force_col_wise=True,          # ✅ CPU 上更稳更省内存\n    **DEVICE_KW,                  # ✅ 正确设定 CPU/GPU\n    verbosity=-1,\n)\n\n# 稍更“竞赛友好”的直方图/抽样设置（速度与稳健折中）\nbase_params.update(dict(\n    feature_fraction=0.75,\n    bagging_fraction=0.80,\n    min_data_in_leaf=60,\n))\n\n# 极速模式（可能略降分）\nif ULTRA_FAST:\n    FOLDS = 3\n    EARLY_STOP = 100\n    base_params.update(dict(\n        learning_rate=0.05,\n        num_leaves=48,\n        min_data_in_leaf=80,\n        feature_fraction=0.70,\n        bagging_fraction=0.75,\n        n_estimators=1500,\n        max_bin=255,\n    ))\n\n# ----------------------- Helpers -----------------------\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\ndef best_iter_of(model: lgb.LGBMClassifier) -> int:\n    bi = getattr(model, \"best_iteration_\", None)\n    if bi is not None and bi > 0:\n        return int(bi)\n    return int(getattr(model, \"n_estimators\", 2000))\n\ndef fit_with_es(model, X_tr, y_tr, X_va, y_va, cat_cols, early_stopping_rounds, verbose):\n    \"\"\"兼容不同 LightGBM 版本的早停写法（优先用 callbacks）。\"\"\"\n    try:\n        # 新写法：callbacks\n        return model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"auc\",\n            categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\",\n            callbacks=[\n                lgb.early_stopping(early_stopping_rounds, verbose=verbose),\n                lgb.log_evaluation(period=50 if verbose else 0),\n            ]\n        )\n    except TypeError:\n        # 旧版本兜底\n        return model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"auc\",\n            categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\",\n            early_stopping_rounds=early_stopping_rounds,\n            verbose=verbose\n        )\n\ndef run_fold(tr_idx, va_idx):\n    model = lgb.LGBMClassifier(**base_params)\n    model = fit_with_es(model,\n                        X.iloc[tr_idx], y[tr_idx],\n                        X.iloc[va_idx], y[va_idx],\n                        cat_cols=cat_cols,\n                        early_stopping_rounds=EARLY_STOP,\n                        verbose=VERBOSE)\n    proba_va = model.predict_proba(X.iloc[va_idx])[:, 1]\n    return va_idx, proba_va, model\n\n# ----------------------- OOF CV（可外层并行） -----------------------\nfolds = list(skf.split(X, y))\nif OUTER_JOBS > 1:\n    results = Parallel(n_jobs=OUTER_JOBS, prefer=\"threads\")(delayed(run_fold)(tr, va) for tr, va in folds)\nelse:\n    results = [run_fold(tr, va) for tr, va in folds]\n\noof = np.zeros(len(y), dtype=float)\nbest_iters = []\nfor va_idx, proba_va, model in results:\n    oof[va_idx] = proba_va\n    bi = best_iter_of(model)\n    best_iters.append(bi)\n    print(f\"[Fold] best_iter={bi}  AUC={roc_auc_score(y[va_idx], oof[va_idx]):.4f}\")\n\nauc = roc_auc_score(y, oof)\nf1_05 = f1_score(y, (oof >= 0.5).astype(int))\nprec, rec, thr = precision_recall_curve(y, oof)\nf1s = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-12)\nbest_ix = int(np.nanargmax(f1s))\nbest_thr = float(thr[best_ix])\nprint(f\"[OOF] AUC={auc:.4f} | F1@0.5={f1_05:.4f} | BestF1={f1s[best_ix]:.4f} @thr={best_thr:.4f}\")\n\n# ----------------------- Full fit with tuned n_estimators -----------------------\nbest_round = max(100, int(np.median(best_iters)))\nprint(f\"[Full] training with n_estimators={best_round} (median of CV best iters)\")\n\nparams_full = dict(base_params)\nparams_full[\"n_estimators\"] = best_round\n\nfinal = lgb.LGBMClassifier(**params_full)\n\n# 用随机 5% 作为早停验证（更快；若不稳定可改 10%）\nrng = np.random.RandomState(SEED)\nidx = rng.permutation(len(X))\ncut = int(0.95 * len(X))\ntr_idx, va_idx = idx[:cut], idx[cut:]\n\nfinal = fit_with_es(final,\n                    X.iloc[tr_idx], y[tr_idx],\n                    X.iloc[va_idx], y[va_idx],\n                    cat_cols=cat_cols,\n                    early_stopping_rounds=EARLY_STOP,\n                    verbose=VERBOSE)\n\ntest_proba = final.predict_proba(X_test)[:, 1]\n\n# ----------------------- Submit -----------------------\nout = sub.copy()\nout[\"y\"] = test_proba\nout.to_csv(\"submission_lgbm_sota.csv\", index=False)\nprint(\"Saved -> submission_lgbm_sota.csv\")\nprint(out.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T02:27:05.549354Z","iopub.execute_input":"2025-08-29T02:27:05.549913Z","iopub.status.idle":"2025-08-29T02:34:49.234097Z","shell.execute_reply.started":"2025-08-29T02:27:05.549888Z","shell.execute_reply":"2025-08-29T02:34:49.233403Z"}},"outputs":[{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[Fold] best_iter=1925  AUC=0.8605\n[Fold] best_iter=1535  AUC=0.8566\n[Fold] best_iter=1229  AUC=0.8563\n[Fold] best_iter=1827  AUC=0.8579\n[Fold] best_iter=1363  AUC=0.8558\n[OOF] AUC=0.8574 | F1@0.5=0.4749 | BestF1=0.5328 @thr=0.6971\n[Full] training with n_estimators=1535 (median of CV best iters)\nSaved -> submission_lgbm_sota.csv\n       id         y\n0  750000  0.216924\n1  750001  0.334075\n2  750002  0.348150\n3  750003  0.002067\n4  750004  0.659697\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}