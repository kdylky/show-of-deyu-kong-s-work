{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T22:37:13.491999Z","iopub.execute_input":"2025-08-28T22:37:13.492416Z","iopub.status.idle":"2025-08-28T22:37:16.674152Z","shell.execute_reply.started":"2025-08-28T22:37:13.492379Z","shell.execute_reply":"2025-08-28T22:37:16.673297Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e8/sample_submission.csv\n/kaggle/input/playground-series-s5e8/train.csv\n/kaggle/input/playground-series-s5e8/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nimport polars as pl\n\nBASE = Path(\"/kaggle/input/playground-series-s5e8\")\ntrain, test, sub = (pl.read_csv(BASE / f) for f in (\"train.csv\", \"test.csv\", \"sample_submission.csv\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T22:40:14.823537Z","iopub.execute_input":"2025-08-28T22:40:14.823837Z","iopub.status.idle":"2025-08-28T22:40:17.369017Z","shell.execute_reply.started":"2025-08-28T22:40:14.823813Z","shell.execute_reply":"2025-08-28T22:40:17.367954Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T22:40:27.019108Z","iopub.execute_input":"2025-08-28T22:40:27.019449Z","iopub.status.idle":"2025-08-28T22:40:27.054486Z","shell.execute_reply.started":"2025-08-28T22:40:27.019426Z","shell.execute_reply":"2025-08-28T22:40:27.053473Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"shape: (750_000, 18)\n┌────────┬─────┬─────────────┬──────────┬───┬───────┬──────────┬──────────┬─────┐\n│ id     ┆ age ┆ job         ┆ marital  ┆ … ┆ pdays ┆ previous ┆ poutcome ┆ y   │\n│ ---    ┆ --- ┆ ---         ┆ ---      ┆   ┆ ---   ┆ ---      ┆ ---      ┆ --- │\n│ i64    ┆ i64 ┆ str         ┆ str      ┆   ┆ i64   ┆ i64      ┆ str      ┆ i64 │\n╞════════╪═════╪═════════════╪══════════╪═══╪═══════╪══════════╪══════════╪═════╡\n│ 0      ┆ 42  ┆ technician  ┆ married  ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 1      ┆ 38  ┆ blue-collar ┆ married  ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 2      ┆ 36  ┆ blue-collar ┆ married  ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 3      ┆ 27  ┆ student     ┆ single   ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 4      ┆ 26  ┆ technician  ┆ married  ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 1   │\n│ …      ┆ …   ┆ …           ┆ …        ┆ … ┆ …     ┆ …        ┆ …        ┆ …   │\n│ 749995 ┆ 29  ┆ services    ┆ single   ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 1   │\n│ 749996 ┆ 69  ┆ retired     ┆ divorced ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 749997 ┆ 50  ┆ blue-collar ┆ married  ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 749998 ┆ 32  ┆ technician  ┆ married  ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 749999 ┆ 42  ┆ technician  ┆ married  ┆ … ┆ 1     ┆ 7        ┆ failure  ┆ 0   │\n└────────┴─────┴─────────────┴──────────┴───┴───────┴──────────┴──────────┴─────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (750_000, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>age</th><th>job</th><th>marital</th><th>education</th><th>default</th><th>balance</th><th>housing</th><th>loan</th><th>contact</th><th>day</th><th>month</th><th>duration</th><th>campaign</th><th>pdays</th><th>previous</th><th>poutcome</th><th>y</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>42</td><td>&quot;technician&quot;</td><td>&quot;married&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>7</td><td>&quot;no&quot;</td><td>&quot;no&quot;</td><td>&quot;cellular&quot;</td><td>25</td><td>&quot;aug&quot;</td><td>117</td><td>3</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>0</td></tr><tr><td>1</td><td>38</td><td>&quot;blue-collar&quot;</td><td>&quot;married&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>514</td><td>&quot;no&quot;</td><td>&quot;no&quot;</td><td>&quot;unknown&quot;</td><td>18</td><td>&quot;jun&quot;</td><td>185</td><td>1</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>0</td></tr><tr><td>2</td><td>36</td><td>&quot;blue-collar&quot;</td><td>&quot;married&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>602</td><td>&quot;yes&quot;</td><td>&quot;no&quot;</td><td>&quot;unknown&quot;</td><td>14</td><td>&quot;may&quot;</td><td>111</td><td>2</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>0</td></tr><tr><td>3</td><td>27</td><td>&quot;student&quot;</td><td>&quot;single&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>34</td><td>&quot;yes&quot;</td><td>&quot;no&quot;</td><td>&quot;unknown&quot;</td><td>28</td><td>&quot;may&quot;</td><td>10</td><td>2</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>0</td></tr><tr><td>4</td><td>26</td><td>&quot;technician&quot;</td><td>&quot;married&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>889</td><td>&quot;yes&quot;</td><td>&quot;no&quot;</td><td>&quot;cellular&quot;</td><td>3</td><td>&quot;feb&quot;</td><td>902</td><td>1</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>1</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>749995</td><td>29</td><td>&quot;services&quot;</td><td>&quot;single&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>1282</td><td>&quot;no&quot;</td><td>&quot;yes&quot;</td><td>&quot;unknown&quot;</td><td>4</td><td>&quot;jul&quot;</td><td>1006</td><td>2</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>1</td></tr><tr><td>749996</td><td>69</td><td>&quot;retired&quot;</td><td>&quot;divorced&quot;</td><td>&quot;tertiary&quot;</td><td>&quot;no&quot;</td><td>631</td><td>&quot;no&quot;</td><td>&quot;no&quot;</td><td>&quot;cellular&quot;</td><td>19</td><td>&quot;aug&quot;</td><td>87</td><td>1</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>0</td></tr><tr><td>749997</td><td>50</td><td>&quot;blue-collar&quot;</td><td>&quot;married&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>217</td><td>&quot;yes&quot;</td><td>&quot;no&quot;</td><td>&quot;cellular&quot;</td><td>17</td><td>&quot;apr&quot;</td><td>113</td><td>1</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>0</td></tr><tr><td>749998</td><td>32</td><td>&quot;technician&quot;</td><td>&quot;married&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>-274</td><td>&quot;no&quot;</td><td>&quot;no&quot;</td><td>&quot;cellular&quot;</td><td>26</td><td>&quot;aug&quot;</td><td>108</td><td>6</td><td>-1</td><td>0</td><td>&quot;unknown&quot;</td><td>0</td></tr><tr><td>749999</td><td>42</td><td>&quot;technician&quot;</td><td>&quot;married&quot;</td><td>&quot;secondary&quot;</td><td>&quot;no&quot;</td><td>1559</td><td>&quot;no&quot;</td><td>&quot;no&quot;</td><td>&quot;cellular&quot;</td><td>4</td><td>&quot;aug&quot;</td><td>143</td><td>1</td><td>1</td><td>7</td><td>&quot;failure&quot;</td><td>0</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import polars as pl\nfrom collections import Counter\nfrom itertools import combinations\nimport math\n\ndef eda_bank_pl(df: pl.DataFrame, top_corr: int = 10):\n    ID, TARGET = \"id\", \"y\"\n    CATS = [c for c in (\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\") if c in df.columns]\n    NUMS = [c for c in (\"age\",\"balance\",\"day\",\"duration\",\"campaign\",\"pdays\",\"previous\") if c in df.columns]  # duration 可能泄漏\n\n    # 1) 基本信息\n    dtype_counts = dict(Counter(map(str, df.dtypes)))\n    print(\"▶ SHAPE / MEM / DTYPES\")\n    print({\"shape\": (df.height, df.width),\n           \"mem_MB\": round(df.estimated_size() / 2**20, 2),\n           \"dtypes_counts\": dtype_counts})\n    print(\"\\n▶ HEAD(3)\")\n    print(df.head(3))\n\n    # 2) 缺失率 & 基数\n    miss = (\n        df.null_count()\n          .transpose(include_header=True, header_name=\"col\", column_names=[\"n_miss\"])\n          .with_columns(\n              pl.col(\"n_miss\").cast(pl.UInt64),\n              (pl.col(\"n_miss\") / df.height * 100).round(2).alias(\"miss_%\")\n          )\n          .filter(pl.col(\"n_miss\") > 0)\n          .sort(\"miss_%\", descending=True)\n    )\n    if miss.height:\n        print(\"\\n▶ MISSING % (non-zero)\")\n        print(miss)\n\n    nunique = (\n        df.select(pl.all().n_unique())\n          .transpose(include_header=True, header_name=\"col\", column_names=[\"nunique\"])\n          .sort(\"nunique\", descending=True)\n    )\n    print(\"\\n▶ NUNIQUE (all cols)\")\n    print(nunique)\n\n    dup = df.height - df.unique().height\n    print(\"\\n▶ DUPLICATES (rows):\", int(dup))\n\n    # 3) 数值特征概要 & 相关性\n    if NUMS:\n        print(\"\\n▶ NUMERIC SUMMARY\")\n        print(df.select(NUMS).describe())\n\n        if len(NUMS) >= 2:\n            pairs = []\n            num_df = df.select(NUMS)\n            for a, b in combinations(NUMS, 2):\n                r = num_df.select(pl.corr(a, b)).to_series().item()\n                r = 0.0 if r is None or (isinstance(r, float) and math.isnan(r)) else float(r)\n                pairs.append({\"A\": a, \"B\": b, \"|r|\": abs(r), \"r\": r})\n            top = pl.DataFrame(pairs).sort(\"|r|\", descending=True).head(top_corr)\n            print(f\"\\n▶ TOP {top_corr} |corr| PAIRS (numeric)\")\n            print(top)\n\n    # 4) 类别特征：基数与与目标的关联\n    if CATS:\n        card = (\n            df.select(CATS).select(pl.all().n_unique())\n              .transpose(include_header=True, header_name=\"col\", column_names=[\"nunique\"])\n              .sort(\"nunique\")\n        )\n        print(\"\\n▶ CATEGORICAL CARDINALITY (low→high)\")\n        print(card)\n\n        if TARGET in df.columns:\n            for c in CATS:\n                out = (\n                    df.group_by(c)\n                      .agg(n=pl.len(), rate=pl.col(TARGET).mean())\n                      .sort([\"rate\",\"n\"], descending=[True, True])\n                      .head(6)\n                )\n                print(f\"\\n▶ {c} vs {TARGET} (top by rate)\")\n                print(out)\n\n    # 5) 目标分布\n    if TARGET in df.columns:\n        counts = df.group_by(TARGET).len().sort(TARGET)\n        ctr = float(df.select(pl.col(TARGET).mean()).to_series().item())\n        print(f\"\\n▶ TARGET '{TARGET}' counts\")\n        print(counts)\n        print(f\"CTR (mean of {TARGET}): {ctr:.4f}\")\n\n\neda_bank_pl(train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T22:52:18.062476Z","iopub.execute_input":"2025-08-28T22:52:18.063458Z","iopub.status.idle":"2025-08-28T22:52:19.223582Z","shell.execute_reply.started":"2025-08-28T22:52:18.063402Z","shell.execute_reply":"2025-08-28T22:52:19.222191Z"}},"outputs":[{"name":"stdout","text":"▶ SHAPE / MEM / DTYPES\n{'shape': (750000, 18), 'mem_MB': 86.64, 'dtypes_counts': {'Int64': 9, 'String': 9}}\n\n▶ HEAD(3)\nshape: (3, 18)\n┌─────┬─────┬─────────────┬─────────┬───┬───────┬──────────┬──────────┬─────┐\n│ id  ┆ age ┆ job         ┆ marital ┆ … ┆ pdays ┆ previous ┆ poutcome ┆ y   │\n│ --- ┆ --- ┆ ---         ┆ ---     ┆   ┆ ---   ┆ ---      ┆ ---      ┆ --- │\n│ i64 ┆ i64 ┆ str         ┆ str     ┆   ┆ i64   ┆ i64      ┆ str      ┆ i64 │\n╞═════╪═════╪═════════════╪═════════╪═══╪═══════╪══════════╪══════════╪═════╡\n│ 0   ┆ 42  ┆ technician  ┆ married ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 1   ┆ 38  ┆ blue-collar ┆ married ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n│ 2   ┆ 36  ┆ blue-collar ┆ married ┆ … ┆ -1    ┆ 0        ┆ unknown  ┆ 0   │\n└─────┴─────┴─────────────┴─────────┴───┴───────┴──────────┴──────────┴─────┘\n\n▶ NUNIQUE (all cols)\nshape: (18, 2)\n┌──────────┬─────────┐\n│ col      ┆ nunique │\n│ ---      ┆ ---     │\n│ str      ┆ u32     │\n╞══════════╪═════════╡\n│ id       ┆ 750000  │\n│ balance  ┆ 8217    │\n│ duration ┆ 1760    │\n│ pdays    ┆ 596     │\n│ age      ┆ 78      │\n│ …        ┆ …       │\n│ contact  ┆ 3       │\n│ default  ┆ 2       │\n│ housing  ┆ 2       │\n│ loan     ┆ 2       │\n│ y        ┆ 2       │\n└──────────┴─────────┘\n\n▶ DUPLICATES (rows): 0\n\n▶ NUMERIC SUMMARY\nshape: (9, 8)\n┌────────────┬───────────┬─────────────┬───────────┬────────────┬──────────┬───────────┬──────────┐\n│ statistic  ┆ age       ┆ balance     ┆ day       ┆ duration   ┆ campaign ┆ pdays     ┆ previous │\n│ ---        ┆ ---       ┆ ---         ┆ ---       ┆ ---        ┆ ---      ┆ ---       ┆ ---      │\n│ str        ┆ f64       ┆ f64         ┆ f64       ┆ f64        ┆ f64      ┆ f64       ┆ f64      │\n╞════════════╪═══════════╪═════════════╪═══════════╪════════════╪══════════╪═══════════╪══════════╡\n│ count      ┆ 750000.0  ┆ 750000.0    ┆ 750000.0  ┆ 750000.0   ┆ 750000.0 ┆ 750000.0  ┆ 750000.0 │\n│ null_count ┆ 0.0       ┆ 0.0         ┆ 0.0       ┆ 0.0        ┆ 0.0      ┆ 0.0       ┆ 0.0      │\n│ mean       ┆ 40.926395 ┆ 1204.067397 ┆ 16.117209 ┆ 256.229144 ┆ 2.577008 ┆ 22.412733 ┆ 0.298545 │\n│ std        ┆ 10.098829 ┆ 2836.096759 ┆ 8.250832  ┆ 272.555662 ┆ 2.718514 ┆ 77.319998 ┆ 1.335926 │\n│ min        ┆ 18.0      ┆ -8019.0     ┆ 1.0       ┆ 1.0        ┆ 1.0      ┆ -1.0      ┆ 0.0      │\n│ 25%        ┆ 33.0      ┆ 0.0         ┆ 9.0       ┆ 91.0       ┆ 1.0      ┆ -1.0      ┆ 0.0      │\n│ 50%        ┆ 39.0      ┆ 634.0       ┆ 17.0      ┆ 133.0      ┆ 2.0      ┆ -1.0      ┆ 0.0      │\n│ 75%        ┆ 48.0      ┆ 1390.0      ┆ 21.0      ┆ 361.0      ┆ 3.0      ┆ -1.0      ┆ 0.0      │\n│ max        ┆ 95.0      ┆ 99717.0     ┆ 31.0      ┆ 4918.0     ┆ 63.0     ┆ 871.0     ┆ 200.0    │\n└────────────┴───────────┴─────────────┴───────────┴────────────┴──────────┴───────────┴──────────┘\n\n▶ TOP 10 |corr| PAIRS (numeric)\nshape: (10, 4)\n┌──────────┬──────────┬──────────┬───────────┐\n│ A        ┆ B        ┆ |r|      ┆ r         │\n│ ---      ┆ ---      ┆ ---      ┆ ---       │\n│ str      ┆ str      ┆ f64      ┆ f64       │\n╞══════════╪══════════╪══════════╪═══════════╡\n│ pdays    ┆ previous ┆ 0.561839 ┆ 0.561839  │\n│ day      ┆ campaign ┆ 0.178806 ┆ 0.178806  │\n│ balance  ┆ duration ┆ 0.109629 ┆ 0.109629  │\n│ day      ┆ pdays    ┆ 0.086197 ┆ -0.086197 │\n│ duration ┆ campaign ┆ 0.083016 ┆ -0.083016 │\n│ age      ┆ balance  ┆ 0.062838 ┆ 0.062838  │\n│ campaign ┆ pdays    ┆ 0.061465 ┆ -0.061465 │\n│ day      ┆ duration ┆ 0.056755 ┆ -0.056755 │\n│ day      ┆ previous ┆ 0.051082 ┆ -0.051082 │\n│ duration ┆ pdays    ┆ 0.047555 ┆ 0.047555  │\n└──────────┴──────────┴──────────┴───────────┘\n\n▶ CATEGORICAL CARDINALITY (low→high)\nshape: (9, 2)\n┌───────────┬─────────┐\n│ col       ┆ nunique │\n│ ---       ┆ ---     │\n│ str       ┆ u32     │\n╞═══════════╪═════════╡\n│ default   ┆ 2       │\n│ housing   ┆ 2       │\n│ loan      ┆ 2       │\n│ marital   ┆ 3       │\n│ contact   ┆ 3       │\n│ education ┆ 4       │\n│ poutcome  ┆ 4       │\n│ job       ┆ 12      │\n│ month     ┆ 12      │\n└───────────┴─────────┘\n\n▶ job vs y (top by rate)\nshape: (6, 3)\n┌───────────────┬────────┬──────────┐\n│ job           ┆ n      ┆ rate     │\n│ ---           ┆ ---    ┆ ---      │\n│ str           ┆ u32    ┆ f64      │\n╞═══════════════╪════════╪══════════╡\n│ student       ┆ 11767  ┆ 0.340784 │\n│ retired       ┆ 35185  ┆ 0.246241 │\n│ unemployed    ┆ 17634  ┆ 0.179823 │\n│ management    ┆ 175541 ┆ 0.150392 │\n│ self-employed ┆ 19020  ┆ 0.129443 │\n│ unknown       ┆ 2917   ┆ 0.120672 │\n└───────────────┴────────┴──────────┘\n\n▶ marital vs y (top by rate)\nshape: (3, 3)\n┌──────────┬────────┬──────────┐\n│ marital  ┆ n      ┆ rate     │\n│ ---      ┆ ---    ┆ ---      │\n│ str      ┆ u32    ┆ f64      │\n╞══════════╪════════╪══════════╡\n│ single   ┆ 194834 ┆ 0.170453 │\n│ divorced ┆ 74407  ┆ 0.111576 │\n│ married  ┆ 480759 ┆ 0.101872 │\n└──────────┴────────┴──────────┘\n\n▶ education vs y (top by rate)\nshape: (4, 3)\n┌───────────┬────────┬──────────┐\n│ education ┆ n      ┆ rate     │\n│ ---       ┆ ---    ┆ ---      │\n│ str       ┆ u32    ┆ f64      │\n╞═══════════╪════════╪══════════╡\n│ tertiary  ┆ 227508 ┆ 0.162649 │\n│ unknown   ┆ 21299  ┆ 0.133387 │\n│ secondary ┆ 401683 ┆ 0.105491 │\n│ primary   ┆ 99510  ┆ 0.083097 │\n└───────────┴────────┴──────────┘\n\n▶ default vs y (top by rate)\nshape: (2, 3)\n┌─────────┬────────┬──────────┐\n│ default ┆ n      ┆ rate     │\n│ ---     ┆ ---    ┆ ---      │\n│ str     ┆ u32    ┆ f64      │\n╞═════════╪════════╪══════════╡\n│ no      ┆ 737151 ┆ 0.121947 │\n│ yes     ┆ 12849  ┆ 0.046307 │\n└─────────┴────────┴──────────┘\n\n▶ housing vs y (top by rate)\nshape: (2, 3)\n┌─────────┬────────┬──────────┐\n│ housing ┆ n      ┆ rate     │\n│ ---     ┆ ---    ┆ ---      │\n│ str     ┆ u32    ┆ f64      │\n╞═════════╪════════╪══════════╡\n│ no      ┆ 338712 ┆ 0.175778 │\n│ yes     ┆ 411288 ┆ 0.075251 │\n└─────────┴────────┴──────────┘\n\n▶ loan vs y (top by rate)\nshape: (2, 3)\n┌──────┬────────┬──────────┐\n│ loan ┆ n      ┆ rate     │\n│ ---  ┆ ---    ┆ ---      │\n│ str  ┆ u32    ┆ f64      │\n╞══════╪════════╪══════════╡\n│ no   ┆ 645023 ┆ 0.131378 │\n│ yes  ┆ 104977 ┆ 0.054736 │\n└──────┴────────┴──────────┘\n\n▶ contact vs y (top by rate)\nshape: (3, 3)\n┌───────────┬────────┬──────────┐\n│ contact   ┆ n      ┆ rate     │\n│ ---       ┆ ---    ┆ ---      │\n│ str       ┆ u32    ┆ f64      │\n╞═══════════╪════════╪══════════╡\n│ cellular  ┆ 486655 ┆ 0.156579 │\n│ telephone ┆ 31718  ┆ 0.136799 │\n│ unknown   ┆ 231627 ┆ 0.042953 │\n└───────────┴────────┴──────────┘\n\n▶ month vs y (top by rate)\nshape: (6, 3)\n┌───────┬───────┬──────────┐\n│ month ┆ n     ┆ rate     │\n│ ---   ┆ ---   ┆ ---      │\n│ str   ┆ u32   ┆ f64      │\n╞═══════╪═══════╪══════════╡\n│ mar   ┆ 5802  ┆ 0.571355 │\n│ sep   ┆ 7409  ┆ 0.534755 │\n│ dec   ┆ 2069  ┆ 0.513291 │\n│ oct   ┆ 9204  ┆ 0.490004 │\n│ apr   ┆ 41319 ┆ 0.235654 │\n│ feb   ┆ 37611 ┆ 0.206801 │\n└───────┴───────┴──────────┘\n\n▶ poutcome vs y (top by rate)\nshape: (4, 3)\n┌──────────┬────────┬──────────┐\n│ poutcome ┆ n      ┆ rate     │\n│ ---      ┆ ---    ┆ ---      │\n│ str      ┆ u32    ┆ f64      │\n╞══════════╪════════╪══════════╡\n│ success  ┆ 17691  ┆ 0.764004 │\n│ other    ┆ 14744  ┆ 0.166848 │\n│ failure  ┆ 45115  ┆ 0.132794 │\n│ unknown  ┆ 672450 ┆ 0.101898 │\n└──────────┴────────┴──────────┘\n\n▶ TARGET 'y' counts\nshape: (2, 2)\n┌─────┬────────┐\n│ y   ┆ len    │\n│ --- ┆ ---    │\n│ i64 ┆ u32    │\n╞═════╪════════╡\n│ 0   ┆ 659512 │\n│ 1   ┆ 90488  │\n└─────┴────────┘\nCTR (mean of y): 0.1207\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ===== Decision Tree x3 (gini / entropy / log_loss) =====\nfrom pathlib import Path\nimport polars as pl\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score\n\n# --- config ---\nBASE = Path(\"/kaggle/input/playground-series-s5e8\")\nTARGET, ID = \"y\", \"id\"\nDROP_DURATION = True          # 是否丢弃潜在泄漏特征 'duration'\nSUBMIT_PROBA  = True          # True=提交概率；False=提交0/1\nRANDOM_STATE  = 42\nCRITERIA = [\"gini\", \"entropy\", \"log_loss\"]\n\n# --- load with polars (fast), then to pandas for sklearn ---\ntrain_pl = pl.read_csv(BASE / \"train.csv\")\ntest_pl  = pl.read_csv(BASE / \"test.csv\")\nsub_pd   = pd.read_csv(BASE / \"sample_submission.csv\")\n\nif DROP_DURATION and \"duration\" in train_pl.columns:\n    train_pl = train_pl.drop(\"duration\")\n    if \"duration\" in test_pl.columns:\n        test_pl = test_pl.drop(\"duration\")\n\ntrain = train_pl.to_pandas()\ntest  = test_pl.to_pandas()\n\n# --- split X/y ---\ny = train[TARGET].astype(int).values\nX = train.drop(columns=[TARGET, ID])\nX_test = test.drop(columns=[ID], errors=\"ignore\")\n\n# --- columns ---\ncat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\nnum_cols = [c for c in X.columns if c not in cat_cols]\n\n# --- preprocessor: OneHot(dense) + passthrough numeric ---\npre = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n        (\"num\", \"passthrough\", num_cols),\n    ],\n    remainder=\"drop\",\n)\n\ndef run_one(criterion: str):\n    print(f\"\\n=== Criterion: {criterion} ===\")\n    clf = DecisionTreeClassifier(\n        criterion=criterion,\n        random_state=RANDOM_STATE,\n        max_depth=8,\n        min_samples_leaf=100,\n        class_weight=\"balanced\",\n    )\n    pipe = Pipeline([(\"prep\", pre), (\"clf\", clf)])\n\n    # 5-fold OOF\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n    oof = np.zeros(len(y), dtype=float)\n    for tr_idx, va_idx in skf.split(X, y):\n        pipe.fit(X.iloc[tr_idx], y[tr_idx])\n        oof[va_idx] = pipe.predict_proba(X.iloc[va_idx])[:, 1]\n\n    auc = roc_auc_score(y, oof)\n    f1  = f1_score(y, (oof >= 0.5).astype(int))\n    print(f\"[CV] AUC={auc:.4f} | F1@0.5={f1:.4f}\")\n\n    # fit full & predict test\n    pipe.fit(X, y)\n    test_proba = pipe.predict_proba(X_test)[:, 1]\n    test_pred  = (test_proba >= 0.5).astype(int)\n\n    out = sub_pd.copy()\n    out[\"y\"] = test_proba if SUBMIT_PROBA else test_pred\n    out_path = f\"submission_{criterion}.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"Saved -> {out_path}\")\n    return {\"criterion\": criterion, \"auc\": auc, \"f1@0.5\": f1, \"file\": out_path}\n\n# --- run all three ---\nresults = [run_one(c) for c in CRITERIA]\n\n# --- summary ---\nprint(\"\\n=== Summary ===\")\nfor r in results:\n    print(f\"{r['criterion']:<8}  AUC={r['auc']:.4f}  F1@0.5={r['f1@0.5']:.4f}  -> {r['file']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T00:10:12.091458Z","iopub.execute_input":"2025-08-29T00:10:12.091818Z","iopub.status.idle":"2025-08-29T00:12:32.795576Z","shell.execute_reply.started":"2025-08-29T00:10:12.091788Z","shell.execute_reply":"2025-08-29T00:12:32.794530Z"}},"outputs":[{"name":"stdout","text":"\n=== Criterion: gini ===\n[CV] AUC=0.8103 | F1@0.5=0.4092\nSaved -> submission_gini.csv\n\n=== Criterion: entropy ===\n[CV] AUC=0.8111 | F1@0.5=0.4088\nSaved -> submission_entropy.csv\n\n=== Criterion: log_loss ===\n[CV] AUC=0.8111 | F1@0.5=0.4088\nSaved -> submission_log_loss.csv\n\n=== Summary ===\ngini      AUC=0.8103  F1@0.5=0.4092  -> submission_gini.csv\nentropy   AUC=0.8111  F1@0.5=0.4088  -> submission_entropy.csv\nlog_loss  AUC=0.8111  F1@0.5=0.4088  -> submission_log_loss.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===== ID3Fast (vectorized, multiway, information gain) with CV + submission =====\nfrom pathlib import Path\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom joblib import Parallel, delayed\n\n# --- config ---\nBASE = Path(\"/kaggle/input/playground-series-s5e8\")\nTARGET, ID = \"y\", \"id\"\nDROP_DURATION = True          # 是否丢弃潜在泄漏特征 'duration'\nRANDOM_STATE  = 42\nN_SPLITS      = 5\nSUBMIT_PROBA  = True          # 提交概率（比赛常用）\nN_JOBS        = -1            # 并行折数\n\n# ID3 超参\nMAX_DEPTH         = 8\nMIN_SAMPLES_LEAF  = 100       # 子节点最少样本\nN_BINS_NUMERIC    = 10        # 数值分箱数（等分位）\nSPECIAL_PDAYS     = True      # pdays == -1 单独成类\nMIN_GAIN          = 1e-6      # 最小信息增益阈值（过小则不分裂，省时）\n\n# ---------- 工具 ----------\ndef entropy_from_pos_neg(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n    n = pos + neg\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        p = np.where(n > 0, pos / n, 0.0)\n        q = 1.0 - p\n        ent = -(np.where(p > 0, p * np.log2(p), 0.0) + np.where(q > 0, q * np.log2(q), 0.0))\n    return ent\n\ndef parent_entropy(y: np.ndarray) -> float:\n    pos = y.sum()\n    neg = len(y) - pos\n    return float(entropy_from_pos_neg(np.array([pos]), np.array([neg]))[0])\n\ndef calc_info_gain(y: np.ndarray, codes: np.ndarray, K: int, min_leaf: int) -> tuple[float, np.ndarray]:\n    \"\"\"返回 (gain, big_idx)；big_idx 是满足 min_leaf 的组索引。\n       仅基于 codes 有效的样本（codes>=0）计算父熵与子熵，避免权重偏移。\n    \"\"\"\n    mask = codes >= 0\n    if not mask.any():\n        return 0.0, np.array([], dtype=int)\n\n    y_eff = y[mask]\n    g = codes[mask]\n\n    # 每组总数/正数\n    cnt  = np.bincount(g, minlength=K)\n    posf = np.bincount(g, weights=y_eff, minlength=K)\n    pos  = posf.astype(np.int64)\n\n    big  = np.where(cnt >= min_leaf)[0]\n    if big.size <= 1:\n        return 0.0, np.array([], dtype=int)\n\n    n_eff = int(mask.sum())\n    ent_parent = parent_entropy(y_eff)\n    ent_child  = entropy_from_pos_neg(pos[big], (cnt[big] - pos[big]))\n    w = cnt[big] / n_eff\n    gain = ent_parent - float(np.sum(w * ent_child))\n    return gain, big\n\n# ---------- ID3 快速版 ----------\nclass ID3Fast:\n    \"\"\"整数编码 + 向量化信息增益 + 批量预测\"\"\"\n    def __init__(self, max_depth=8, min_samples_leaf=100, n_bins_numeric=10,\n                 random_state=42, special_pdays=True, min_gain=1e-6):\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.n_bins_numeric = n_bins_numeric\n        self.random_state = random_state\n        self.special_pdays = special_pdays\n        self.min_gain = min_gain\n\n        # 拟合得到的元信息\n        self.num_edges_ = {}        # 数值列 -> edges (np.ndarray)\n        self.cat_categories_ = {}   # 类别列 -> pandas Index（训练出现过的类别）\n        self.col_names_ = []\n        self.col_is_num_ = []       # bool list\n        self.col_K_ = []            # 每列的类别数（编码后 0..K-1，未知为 -1）\n        self.tree_ = None\n        self.major_proba_ = 0.0     # 根节点正类概率（未知回退）\n\n    # ---- 编码器：拟合 ----\n    def _fit_encoders(self, X: pd.DataFrame):\n        self.col_names_ = X.columns.tolist()\n        self.col_is_num_.clear()\n        self.num_edges_.clear()\n        self.cat_categories_.clear()\n        self.col_K_.clear()\n\n        for c in self.col_names_:\n            if pd.api.types.is_numeric_dtype(X[c]):\n                self.col_is_num_.append(True)\n                col = X[c].to_numpy()\n                if self.special_pdays and c == \"pdays\":\n                    mask = (col != -1)\n                    edges = self._quantile_edges(col[mask], self.n_bins_numeric) if mask.any() else np.array([])\n                    self.num_edges_[c] = edges\n                    K = max(len(edges) - 1, 0) + 1  # +1 for \"pdays=-1\"\n                else:\n                    edges = self._quantile_edges(col, self.n_bins_numeric)\n                    self.num_edges_[c] = edges\n                    K = max(len(edges) - 1, 0)\n                self.col_K_.append(K)\n            else:\n                self.col_is_num_.append(False)\n                cats = pd.Index(pd.Series(X[c], dtype=\"string\").dropna().unique())\n                self.cat_categories_[c] = cats\n                self.col_K_.append(len(cats))\n\n    @staticmethod\n    def _quantile_edges(x: np.ndarray, n_bins: int) -> np.ndarray:\n        x = x[~np.isnan(x)]\n        if x.size == 0:\n            return np.array([])\n        qs = np.unique(np.quantile(x, np.linspace(0, 1, n_bins + 1)))\n        if qs.size <= 2:\n            lo, hi = np.min(x), np.max(x)\n            qs = np.unique(np.linspace(lo, hi, min(n_bins + 1, int(len(np.unique(x))) + 1)))\n        return qs if qs.size > 1 else np.array([])\n\n    # ---- 编码器：应用（DataFrame -> int 矩阵）----\n    def _encode_df(self, X: pd.DataFrame) -> np.ndarray:\n        Xc = np.empty((len(X), len(self.col_names_)), dtype=np.int32)\n        for j, c in enumerate(self.col_names_):\n            if self.col_is_num_[j]:\n                vals = X[c].to_numpy()\n                if self.special_pdays and c == \"pdays\":\n                    mask = (vals == -1)\n                    edges = self.num_edges_.get(c, np.array([]))\n                    if edges.size < 2:\n                        codes = np.full(len(vals), -1, dtype=np.int32)\n                    else:\n                        codes = np.digitize(vals, edges[1:-1], right=False).astype(np.int32)\n                    K_base = max(len(edges) - 1, 0)\n                    codes[mask] = K_base  # 特殊类\n                else:\n                    edges = self.num_edges_.get(c, np.array([]))\n                    if edges.size < 2:\n                        codes = np.full(len(vals), -1, dtype=np.int32)\n                    else:\n                        codes = np.digitize(vals, edges[1:-1], right=False).astype(np.int32)\n                Xc[:, j] = codes\n            else:\n                cats = self.cat_categories_.get(c, pd.Index([]))\n                codes = pd.Categorical(pd.Series(X[c], dtype=\"string\"), categories=cats).codes.astype(np.int32)\n                Xc[:, j] = codes  # 未见过的类别为 -1\n        return Xc\n\n    # ---- 训练 ----\n    def fit(self, X: pd.DataFrame, y: np.ndarray):\n        rng = np.random.RandomState(self.random_state)\n        y = y.astype(np.int8, copy=False)\n        self._fit_encoders(X)\n        X_codes = self._encode_df(X)\n\n        # 根节点概率（未知回退）\n        self.major_proba_ = float(y.mean()) if len(y) else 0.0\n\n        feats = list(range(X_codes.shape[1]))\n        rng.shuffle(feats)\n        self.tree_ = self._build(X_codes, y, depth=0, features=feats)\n        return self\n\n    def _build(self, Xc: np.ndarray, y: np.ndarray, depth: int, features: list):\n        # ——护栏：features 必须是全局列索引，Xc 不做列切片——\n        assert Xc.ndim == 2 and all(0 <= f < Xc.shape[1] for f in features), \"Feature index out of range.\"\n\n        node = {\n            \"is_leaf\": False,\n            \"n\": int(len(y)),\n            \"proba\": float(y.mean()) if len(y) else 0.0,\n        }\n        if (depth >= self.max_depth or\n            len(y) < 2 * self.min_samples_leaf or\n            y.min() == y.max() or\n            len(features) == 0):\n            node[\"is_leaf\"] = True\n            return node\n\n        best_feat, best_gain, best_big_idx = None, 0.0, None\n        for f in features:\n            K = self.col_K_[f]\n            if K <= 1:\n                continue\n            gain, big_idx = calc_info_gain(y, Xc[:, f], K, self.min_samples_leaf)\n            if gain > best_gain:\n                best_feat, best_gain, best_big_idx = f, gain, big_idx\n\n        if best_feat is None or best_gain <= self.min_gain or best_big_idx is None or best_big_idx.size == 0:\n            node[\"is_leaf\"] = True\n            return node\n\n        node[\"feat_idx\"] = best_feat\n        node[\"children\"] = {}\n        remaining = [f for f in features if f != best_feat]\n\n        col = Xc[:, best_feat]\n        for val in best_big_idx:\n            idx = (col == val)\n            # 仅切“行”，不切“列” —— 关键修复点\n            child = self._build(Xc[idx], y[idx], depth + 1, remaining)\n            node[\"children\"][int(val)] = child\n        return node\n\n    # ---- 预测（批量路由）----\n    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n        Xc = self._encode_df(X)\n        n = Xc.shape[0]\n        out = np.empty(n, dtype=float)\n\n        def apply_node(node, idxs):\n            if len(idxs) == 0:\n                return\n            if node.get(\"is_leaf\", False) or \"feat_idx\" not in node:\n                out[idxs] = node.get(\"proba\", self.major_proba_)\n                return\n            f = node[\"feat_idx\"]\n            col = Xc[idxs, f]\n\n            # 先给默认（未知或被过滤小类）概率\n            out[idxs] = node.get(\"proba\", self.major_proba_)\n            # 针对每个子值下钻\n            for val, child in node[\"children\"].items():\n                m = (col == val)\n                if m.any():\n                    sub = idxs[m]\n                    apply_node(child, sub)\n\n        apply_node(self.tree_, np.arange(n))\n        out = np.clip(out, 1e-9, 1 - 1e-9)\n        return np.vstack([1 - out, out]).T\n\n# ---------- 数据加载 ----------\ntrain_pl = pl.read_csv(BASE / \"train.csv\")\ntest_pl  = pl.read_csv(BASE / \"test.csv\")\nsub_pd   = pd.read_csv(BASE / \"sample_submission.csv\")\n\nif DROP_DURATION and \"duration\" in train_pl.columns:\n    train_pl = train_pl.drop(\"duration\")\n    if \"duration\" in test_pl.columns:\n        test_pl = test_pl.drop(\"duration\")\n\ntrain = train_pl.to_pandas()\ntest  = test_pl.to_pandas()\n\ny = train[TARGET].astype(int).values\nX = train.drop(columns=[TARGET, ID])\nX_test = test.drop(columns=[ID], errors=\"ignore\")\n\n# ---------- 5 折 OOF（并行） ----------\ndef run_fold(tr_idx, va_idx):\n    clf = ID3Fast(\n        max_depth=MAX_DEPTH,\n        min_samples_leaf=MIN_SAMPLES_LEAF,\n        n_bins_numeric=N_BINS_NUMERIC,\n        random_state=RANDOM_STATE,\n        special_pdays=SPECIAL_PDAYS,\n        min_gain=MIN_GAIN,\n    )\n    clf.fit(X.iloc[tr_idx], y[tr_idx])\n    proba_va = clf.predict_proba(X.iloc[va_idx])[:, 1]\n    return va_idx, proba_va, clf\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\nfolds = list(skf.split(X, y))\nresults = Parallel(n_jobs=N_JOBS, prefer=\"threads\")(delayed(run_fold)(tr, va) for tr, va in folds)\n\noof = np.zeros(len(y), dtype=float)\nfor va_idx, proba_va, _ in results:\n    oof[va_idx] = proba_va\n\nauc = roc_auc_score(y, oof)\nf1  = f1_score(y, (oof >= 0.5).astype(int))\nprint(f\"[ID3Fast][CV] AUC={auc:.4f} | F1@0.5={f1:.4f}\")\n\n# ---------- 全量训练 + 测试提交 ----------\nfinal_clf = ID3Fast(\n    max_depth=MAX_DEPTH,\n    min_samples_leaf=MIN_SAMPLES_LEAF,\n    n_bins_numeric=N_BINS_NUMERIC,\n    random_state=RANDOM_STATE,\n    special_pdays=SPECIAL_PDAYS,\n    min_gain=MIN_GAIN,\n)\nfinal_clf.fit(X, y)\ntest_proba = final_clf.predict_proba(X_test)[:, 1]\ntest_pred  = (test_proba >= 0.5).astype(int)\n\nout = sub_pd.copy()\nout[\"y\"] = test_proba if SUBMIT_PROBA else test_pred\nout.to_csv(\"submission_id3_fast.csv\", index=False)\nprint(\"Saved -> submission_id3_fast.csv\")\nprint(out.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T00:31:16.163381Z","iopub.execute_input":"2025-08-29T00:31:16.163761Z","iopub.status.idle":"2025-08-29T00:31:43.402900Z","shell.execute_reply.started":"2025-08-29T00:31:16.163735Z","shell.execute_reply":"2025-08-29T00:31:43.401575Z"}},"outputs":[{"name":"stdout","text":"[ID3Fast][CV] AUC=0.8095 | F1@0.5=0.3657\nSaved -> submission_id3_fast.csv\n       id         y\n0  750000  0.036364\n1  750001  0.194175\n2  750002  0.102473\n3  750003  0.003040\n4  750004  0.244898\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ===== C4.5 (gain ratio; numeric=binary split, categorical=multiway) — fixed =====\n\n# --- gain ratio 计算（分类：多叉；数值：二分阈值） ---\ndef calc_gain_ratio_multi(y: np.ndarray, codes: np.ndarray, K: int, min_leaf: int) -> tuple[float, np.ndarray]:\n    mask = codes >= 0\n    if not mask.any():\n        return 0.0, np.array([], dtype=int)\n\n    y_eff = y[mask]\n    g = codes[mask]\n\n    cnt  = np.bincount(g, minlength=K)\n    posf = np.bincount(g, weights=y_eff, minlength=K)\n    pos  = posf.astype(np.int64)\n\n    big = np.where(cnt >= min_leaf)[0]\n    if big.size <= 1:\n        return 0.0, np.array([], dtype=int)\n\n    n_eff = int(mask.sum())\n    ent_parent = parent_entropy(y_eff)\n    ent_child  = entropy_from_pos_neg(pos[big], (cnt[big] - pos[big]))\n    w = cnt[big] / n_eff\n    ig = ent_parent - float(np.sum(w * ent_child))\n    split_info = -np.sum(np.where(w > 0, w * np.log2(w), 0.0))\n    if split_info <= 1e-12:\n        return 0.0, big\n    gr = ig / split_info\n    return float(gr), big\n\ndef calc_gain_ratio_threshold(y: np.ndarray, codes: np.ndarray, K: int, min_leaf: int) -> tuple[float, int]:\n    \"\"\"数值列：在编码后的 bin 边界 0..K-2 上枚举二分阈值，返回 (best_gr, best_thr_code)。\"\"\"\n    mask = codes >= 0\n    if not mask.any() or K <= 1:\n        return 0.0, -1\n\n    y_eff = y[mask].astype(np.int32, copy=False)\n    g     = codes[mask].astype(np.int32, copy=False)\n\n    cnt  = np.bincount(g, minlength=K).astype(np.int64)\n    posf = np.bincount(g, weights=y_eff, minlength=K)\n    pos  = posf.astype(np.int64)\n\n    cnt_cum = np.cumsum(cnt)\n    pos_cum = np.cumsum(pos)\n    n_eff = int(cnt_cum[-1])\n\n    best_gr, best_t = 0.0, -1\n    ent_parent = parent_entropy(y_eff)\n\n    for t in range(0, K - 1):\n        lc = int(cnt_cum[t]); rc = int(n_eff - lc)\n        if lc < min_leaf or rc < min_leaf:\n            continue\n        lp = int(pos_cum[t]);  rp = int(pos_cum[-1] - lp)\n\n        ent_left  = entropy_from_pos_neg(np.array([lp]), np.array([lc - lp]))[0]\n        ent_right = entropy_from_pos_neg(np.array([rp]), np.array([rc - rp]))[0]\n\n        wl = lc / n_eff; wr = rc / n_eff\n        ig = ent_parent - (wl * ent_left + wr * ent_right)\n        split_info = - ( (wl * np.log2(wl) if wl > 0 else 0.0) + (wr * np.log2(wr) if wr > 0 else 0.0) )\n        if split_info <= 1e-12:\n            continue\n        gr = ig / split_info\n        if gr > best_gr:\n            best_gr, best_t = float(gr), int(t)\n\n    return best_gr, best_t\n\n# --- C4.5 快速版：继承 ID3Fast 的编码器，使用增益率；数值列保存阈值并正确路由 ---\nclass C45Fast(ID3Fast):\n    def _build(self, Xc: np.ndarray, y: np.ndarray, depth: int, features: list):\n        node = {\"is_leaf\": False, \"n\": int(len(y)), \"proba\": float(y.mean()) if len(y) else 0.0}\n        if (depth >= self.max_depth or len(y) < 2 * self.min_samples_leaf or y.min() == y.max() or len(features) == 0):\n            node[\"is_leaf\"] = True\n            return node\n\n        best_feat, best_score, best_payload = None, 0.0, None\n        for f in features:\n            K = self.col_K_[f]\n            if K <= 1:\n                continue\n            col = Xc[:, f]\n            if self.col_is_num_[f]:\n                score, thr = calc_gain_ratio_threshold(y, col, K, self.min_samples_leaf)\n                if score > best_score:\n                    best_feat, best_score, best_payload = f, score, (\"num\", int(thr))\n            else:\n                score, big_idx = calc_gain_ratio_multi(y, col, K, self.min_samples_leaf)\n                if score > best_score:\n                    best_feat, best_score, best_payload = f, score, (\"cat\", big_idx)\n\n        if best_feat is None or best_score <= self.min_gain or best_payload is None:\n            node[\"is_leaf\"] = True\n            return node\n\n        kind, info = best_payload\n        node[\"feat_idx\"] = best_feat\n        node[\"split_type\"] = kind\n        node[\"children\"] = {}\n        remaining = [f for f in features if f != best_feat]\n        col = Xc[:, best_feat]\n\n        if kind == \"num\":\n            thr = info\n            if thr < 0:\n                node[\"is_leaf\"] = True\n                return node\n            node[\"thr\"] = int(thr)   # ✅ 保存阈值（编码后的 bin 索引）\n            m_le  = (col >= 0) & (col <= thr)\n            m_gt  = (col >  thr)\n            if m_le.sum() >= self.min_samples_leaf:\n                node[\"children\"][\"le\"] = self._build(Xc[m_le], y[m_le], depth + 1, remaining)\n            if m_gt.sum() >= self.min_samples_leaf:\n                node[\"children\"][\"gt\"] = self._build(Xc[m_gt], y[m_gt], depth + 1, remaining)\n        else:\n            big_idx = info\n            for val in big_idx:\n                m = (col == val)\n                if m.sum() >= self.min_samples_leaf:\n                    node[\"children\"][int(val)] = self._build(Xc[m], y[m], depth + 1, remaining)\n        return node\n\n    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n        Xc = self._encode_df(X)\n        n = Xc.shape[0]\n        out = np.empty(n, dtype=float)\n\n        def apply_node(node, idxs):\n            if len(idxs) == 0:\n                return\n            if node.get(\"is_leaf\", False) or \"feat_idx\" not in node:\n                out[idxs] = node.get(\"proba\", self.major_proba_)\n                return\n\n            f = node[\"feat_idx\"]\n            col = Xc[idxs, f]\n            out[idxs] = node.get(\"proba\", self.major_proba_)\n\n            if node.get(\"split_type\") == \"num\":\n                thr = node.get(\"thr\", None)  # ✅ 读取保存的阈值\n                if thr is None:\n                    return\n                m_le = (col >= 0) & (col <= thr)\n                m_gt = (col >  thr)\n                if \"le\" in node[\"children\"] and m_le.any():\n                    apply_node(node[\"children\"][\"le\"], idxs[m_le])\n                if \"gt\" in node[\"children\"] and m_gt.any():\n                    apply_node(node[\"children\"][\"gt\"], idxs[m_gt])\n            else:\n                for val, child in node[\"children\"].items():\n                    m = (col == int(val))\n                    if m.any():\n                        apply_node(child, idxs[m])\n\n        apply_node(self.tree_, np.arange(n))\n        out = np.clip(out, 1e-9, 1 - 1e-9)\n        return np.vstack([1 - out, out]).T\n\n# --- 运行 C4.5（沿用现有 folds 并行） ---\ndef run_fold_c45(tr_idx, va_idx):\n    clf = C45Fast(\n        max_depth=MAX_DEPTH,\n        min_samples_leaf=MIN_SAMPLES_LEAF,\n        n_bins_numeric=N_BINS_NUMERIC,\n        random_state=RANDOM_STATE,\n        special_pdays=SPECIAL_PDAYS,\n        min_gain=MIN_GAIN,\n    )\n    clf.fit(X.iloc[tr_idx], y[tr_idx])\n    proba_va = clf.predict_proba(X.iloc[va_idx])[:, 1]\n    return va_idx, proba_va, clf\n\nresults_c45 = Parallel(n_jobs=N_JOBS, prefer=\"threads\")(delayed(run_fold_c45)(tr, va) for tr, va in folds)\n\noof_c45 = np.zeros(len(y), dtype=float)\nfor va_idx, proba_va, _ in results_c45:\n    oof_c45[va_idx] = proba_va\n\nauc_c45 = roc_auc_score(y, oof_c45)\nf1_c45  = f1_score(y, (oof_c45 >= 0.5).astype(int))\nprint(f\"[C4.5][CV] AUC={auc_c45:.4f} | F1@0.5={f1_c45:.4f}\")\n\n# --- 全量训练 + 提交 ---\nfinal_c45 = C45Fast(\n    max_depth=MAX_DEPTH,\n    min_samples_leaf=MIN_SAMPLES_LEAF,\n    n_bins_numeric=N_BINS_NUMERIC,\n    random_state=RANDOM_STATE,\n    special_pdays=SPECIAL_PDAYS,\n    min_gain=MIN_GAIN,\n)\nfinal_c45.fit(X, y)\ntest_proba_c45 = final_c45.predict_proba(X_test)[:, 1]\ntest_pred_c45  = (test_proba_c45 >= 0.5).astype(int)\n\nout_c45 = sub_pd.copy()\nout_c45[\"y\"] = test_proba_c45 if SUBMIT_PROBA else test_pred_c45\nout_c45.to_csv(\"submission_c45_fast.csv\", index=False)\nprint(\"Saved -> submission_c45_fast.csv\")\nprint(out_c45.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T00:51:03.593120Z","iopub.execute_input":"2025-08-29T00:51:03.593384Z","iopub.status.idle":"2025-08-29T00:51:20.939802Z","shell.execute_reply.started":"2025-08-29T00:51:03.593366Z","shell.execute_reply":"2025-08-29T00:51:20.938608Z"}},"outputs":[{"name":"stdout","text":"[C4.5][CV] AUC=0.8165 | F1@0.5=0.3836\nSaved -> submission_c45_fast.csv\n       id         y\n0  750000  0.045688\n1  750001  0.100774\n2  750002  0.126440\n3  750003  0.003601\n4  750004  0.168831\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}